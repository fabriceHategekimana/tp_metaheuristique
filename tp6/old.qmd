---
title: Genetic algorithms and function minimization
author: Fabrice Hategekimana
format:
  pdf:
    code-fold: true
---

## GA algorithm

Experiment GA algorithm for all combinations of the following cases:

	- p_m = 0.01 or p_m = 0.1
	- no crossover or crossover with p_c = 0.6 (4 cases in total)

For each of the 4 cases, run with/out keeping track of best -> (8 cases in total)

In all cases: N=100, k=20

```{python}
from app import GA, fxy
from itertools import product
import pandas as pd

# population size
N = 100
# individual bit string
k = 20
### Run GA for all above cases 10 times

p_ms = [0.01, 0.1]
crossovers = [False, True]
evals = [10 ** 3, 10 ** 4, 10 ** 5]

cases = list(product(p_ms, crossovers, evals))

df = pd.DataFrame(columns=["p_m", "crossover", "eval", "fit_best", "probs", "probs1", "probs2p5"])
global_fitness = dict()

for p_m, crossover, ev in cases:
	res = GA(N=N, k=k, p_c=0.6, p_m=p_m, evals=ev, crossOver=crossover, track_best=False, f=fxy)
	gen, g_fitness, fit_best, probs, probs1, probs2p5 = res
	global_fitness[f"{p_m}_{crossover}_{ev}"] = g_fitness
	df = df.append(pd.Series((p_m, crossover, ev, fit_best, probs, probs1, probs2p5), index=df.columns), ignore_index=True)

df.to_csv("data_by_cases.csv", index=False)

import json
with open("global_fitness.json", "w") as fichier_json:
    json.dump(global_fitness, fichier_json)
```

## Statistics: 

I tried to register those 4 cases.

1. p_m = 0.01 & no crossover  
2. p_m = 0.01 & p_c = 0.6  
3. p_m = 0.1 & no crossover  
4. p_m = 0.1 & p_c = 0.6  

I ploted the cumulative empirical probabilities (probabilities vs. fitness evaluations) for all 4 cases for:

1. optimum
2. 1% away from optimum
3. 2.5% away from optimum

```{python}
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("data_by_cases.csv")
df = df.sort_values(by="probs")
x = range(len(df))

plt.plot(x, df["probs"])
plt.plot(x, df["probs1"])
plt.plot(x, df["probs2p5"])
plt.xlabel("Configurations")
plt.ylabel("Probability")
plt.title("Probability of the closeness to the optimum")
plt.show()
```

Il y a des configurations(p_m, crossover, fitness evaluation).

```{python}
df
```

I noticed that the most effective configurations are those which have 10^3 in the fitness evaluation. It seems that there is no need to go further to get good results.

I also notice that there is good convergence with the absence of crossover and a mutation probability of 0.10.
There is also on the other hand the existence of a convergence with a crossover and a weak mutation (= 0.01).

I deduce from this that it is rather necessary to favor one of the two parameters (crossover, mutation) and not put a lot of weight on both parameters at the same time.


#### Answer the following Questions: 

1. Q: What is the Search Space of our problem?

   A: The search space of our problem is the set of couple (x, y) contained in the interval x,y \in [10, 1000].


2. Q: What are the roles of Selection, Crossover, & Mutation processes?

   A: The Selection play the role of optimizing the fitness since it select the next generation according to the best fitness. But with the selection only, we dont have the power to explore for more diversity. The Crossover and Mutation processes add some variance to the set of individuals but keep in the same time the probability of the new individuals generated to stay in the subset of the best elements (from the selection step).
    

3. Q: What does each of the 3 processes Selection, Crossover & Mutation favor : exploration/diversification or exploitation/intensification?

   A: The Selection is there for the exploitation but the Crossover and mutation are there for the diversification (exploration).
